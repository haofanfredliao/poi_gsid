import torch
import torch.optim as optim
import os
from tqdm.auto import tqdm


class RQVAETrainer:
    """RQ-VAEè®­ç»ƒå™¨"""
    def __init__(self, config, model, dataloader):
        self.config = config
        self.model = model
        self.dataloader = dataloader
        self.device = config.device
        
        # ä¼˜åŒ–å™¨
        if config.learner == "AdamW":
            self.optimizer = optim.AdamW(
                model.parameters(),
                lr=config.lr,
                weight_decay=config.weight_decay
            )
        elif config.learner == "Adam":
            self.optimizer = optim.Adam(
                model.parameters(),
                lr=config.lr,
                weight_decay=config.weight_decay
            )
        else:
            raise ValueError(f"Unknown optimizer: {config.learner}")
        
        # å­¦ä¹ ç‡è°ƒåº¦å™¨
        if config.lr_scheduler_type == "constant":
            self.scheduler = None
        elif config.lr_scheduler_type == "cosine":
            self.scheduler = optim.lr_scheduler.CosineAnnealingLR(
                self.optimizer,
                T_max=config.epochs
            )
        elif config.lr_scheduler_type == "step":
            self.scheduler = optim.lr_scheduler.StepLR(
                self.optimizer,
                step_size=config.epochs // 3,
                gamma=0.1
            )
        
        # Warmup
        self.warmup_epochs = config.warmup_epochs
        self.base_lr = config.lr
        
        # è®°å½•
        self.history = {
            'epoch': [],
            'train_loss': [],
            'recon_loss': [],
            'quant_loss': [],
            'collision_rate': []
        }
        
        self.best_loss = float('inf')
        self.best_collision_rate = 1.0
        
        print("è®­ç»ƒå™¨åˆå§‹åŒ–å®Œæˆ")
    
    def warmup_lr(self, epoch):
        """Warmupå­¦ä¹ ç‡"""
        if epoch < self.warmup_epochs:
            lr = self.base_lr * (epoch + 1) / self.warmup_epochs
            for param_group in self.optimizer.param_groups:
                param_group['lr'] = lr
    
    def calculate_collision_rate(self, indices):
        """
        è®¡ç®—collision rateï¼ˆé‡å¤ç‡ï¼‰
        
        Args:
            indices: shape (num_samples, num_quantizers)
        
        Returns:
            collision_rate: ç¢°æ’ç‡
        """
        # å°†multi-level indicesè½¬æ¢ä¸ºå•ä¸€ID
        # ä¾‹å¦‚ [a, b, c] -> a * K2*K3 + b * K3 + c
        num_quantizers = indices.shape[1]
        codebook_sizes = self.config.num_emb_list
        
        unique_id = indices[:, 0].clone()
        multiplier = 1
        for i in range(1, num_quantizers):
            multiplier *= codebook_sizes[i-1]
            unique_id = unique_id * codebook_sizes[i] + indices[:, i]
        
        # è®¡ç®—uniqueæ•°é‡
        num_unique = len(torch.unique(unique_id))
        total = len(unique_id)
        
        collision_rate = 1.0 - (num_unique / total)
        return collision_rate
    
    def train_epoch(self, epoch):
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.model.train()
        
        # Warmup
        if epoch < self.warmup_epochs:
            self.warmup_lr(epoch)
        
        total_loss = 0
        total_recon_loss = 0
        total_quant_loss = 0
        all_indices = []
        
        pbar = tqdm(self.dataloader, desc=f"Epoch {epoch+1}/{self.config.epochs}")
        
        for poi_ids, embeddings in pbar:
            embeddings = embeddings.to(self.device)
            
            # Forward
            recon, loss, quant_loss, recon_loss, indices = self.model(embeddings, epoch)
            
            # Backward
            self.optimizer.zero_grad()
            loss.backward()
            self.optimizer.step()
            
            # è®°å½•
            total_loss += loss.item()
            total_recon_loss += recon_loss.item()
            total_quant_loss += quant_loss.item()
            all_indices.append(indices.cpu())
            
            # æ›´æ–°è¿›åº¦æ¡
            pbar.set_postfix({
                'loss': f"{loss.item():.6f}",
                'recon': f"{recon_loss.item():.6f}",
                'quant': f"{quant_loss.item():.6f}"
            })
        
        # å­¦ä¹ ç‡è°ƒåº¦
        if self.scheduler is not None:
            self.scheduler.step()
        
        # è®¡ç®—å¹³å‡å€¼
        num_batches = len(self.dataloader)
        avg_loss = total_loss / num_batches
        avg_recon_loss = total_recon_loss / num_batches
        avg_quant_loss = total_quant_loss / num_batches
        
        # è®¡ç®—collision rate
        all_indices = torch.cat(all_indices, dim=0)
        collision_rate = self.calculate_collision_rate(all_indices)
        
        return avg_loss, avg_recon_loss, avg_quant_loss, collision_rate
    
    def fit(self):
        """å®Œæ•´è®­ç»ƒæµç¨‹"""
        print(f"\n{'='*60}")
        print(f"å¼€å§‹è®­ç»ƒ RQ-VAE")
        print(f"{'='*60}\n")
        
        for epoch in range(self.config.epochs):
            # è®­ç»ƒ
            train_loss, recon_loss, quant_loss, collision_rate = self.train_epoch(epoch)
            
            # è®°å½•
            self.history['epoch'].append(epoch)
            self.history['train_loss'].append(train_loss)
            self.history['recon_loss'].append(recon_loss)
            self.history['quant_loss'].append(quant_loss)
            self.history['collision_rate'].append(collision_rate)
            
            # æ›´æ–°æœ€ä½³ç»“æœ
            if train_loss < self.best_loss:
                self.best_loss = train_loss
            if collision_rate < self.best_collision_rate:
                self.best_collision_rate = collision_rate
            
            # è¯„ä¼°å’Œä¿å­˜
            if (epoch + 1) % self.config.eval_step == 0:
                current_lr = self.optimizer.param_groups[0]['lr']
                print(f"\n{'='*60}")
                print(f"Epoch {epoch+1}/{self.config.epochs}")
                print(f"{'='*60}")
                print(f"ğŸ“Š Train Loss: {train_loss:.6f}")
                print(f"   - Recon Loss: {recon_loss:.6f}")
                print(f"   - Quant Loss: {quant_loss:.6f}")
                print(f"ğŸ¯ Collision Rate: {collision_rate:.4%}")
                print(f"ğŸ“ˆ Learning Rate: {current_lr:.2e}")
                print(f"ğŸ† Best Loss: {self.best_loss:.6f}")
                print(f"ğŸ† Best Collision: {self.best_collision_rate:.4%}")
                
                # ä¿å­˜checkpoint
                self.save_checkpoint(epoch, train_loss, collision_rate)
        
        print(f"\n{'='*60}")
        print(f"âœ… è®­ç»ƒå®Œæˆ!")
        print(f"{'='*60}")
        print(f"ğŸ† Best Loss: {self.best_loss:.6f}")
        print(f"ğŸ† Best Collision Rate: {self.best_collision_rate:.4%}")
        
        return self.best_loss, self.best_collision_rate
    
    def save_checkpoint(self, epoch, loss, collision_rate):
        """ä¿å­˜checkpoint"""
        checkpoint_path = os.path.join(
            self.config.ckpt_dir,
            f"checkpoint_epoch_{epoch+1}_loss_{loss:.6f}.pt"
        )
        
        torch.save({
            'epoch': epoch,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'loss': loss,
            'collision_rate': collision_rate,
            'config': self.config,
            'history': self.history
        }, checkpoint_path)
        
        print(f"ğŸ’¾ Checkpoint saved: {checkpoint_path}")
        
        # ä¿ç•™æœ€è¿‘çš„Nä¸ªcheckpoints
        checkpoints = sorted(
            [f for f in os.listdir(self.config.ckpt_dir) if f.endswith('.pt')],
            key=lambda x: os.path.getmtime(os.path.join(self.config.ckpt_dir, x))
        )
        
        if len(checkpoints) > self.config.save_limit:
            for old_ckpt in checkpoints[:-self.config.save_limit]:
                os.remove(os.path.join(self.config.ckpt_dir, old_ckpt))